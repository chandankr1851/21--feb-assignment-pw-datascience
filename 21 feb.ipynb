{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f24a918-d4f6-4836-aabd-47390cd9fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
    "Q2. What are the different methods used for Web Scraping?\n",
    "Q3. What is Beautiful Soup? Why is it used?\n",
    "Q4. Why is flask used in this Web Scraping project?\n",
    "Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
    "\n",
    "\n",
    "\n",
    "**Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.**\n",
    "- **Web Scraping:** Web scraping is the process of extracting data from websites by fetching and parsing the HTML code of web pages. It involves automatically navigating web pages, extracting desired information, and transforming it into a structured format.\n",
    "\n",
    "- **Purpose:** Web scraping is used to gather data from websites for various purposes, such as research, analysis, monitoring, and automation. It's used when manual data collection is impractical, time-consuming, or not feasible.\n",
    "\n",
    "**Three Areas Where Web Scraping is Used:**\n",
    "1. **E-Commerce:** Web scraping is used to gather product information, prices, reviews, and other data from e-commerce websites to monitor competitor prices, analyze trends, or aggregate product information.\n",
    "2. **Real Estate:** It's used to collect data about property listings, prices, and locations from real estate websites for market analysis and property value estimation.\n",
    "3. **News and Social Media:** Web scraping is used to collect news articles, social media posts, comments, and sentiment analysis for media monitoring, trend analysis, and opinion tracking.\n",
    "\n",
    "**Q2. What are the different methods used for Web Scraping?**\n",
    "Web scraping can be done using various methods, including:\n",
    "1. **Manual Copy-Pasting:** Manually copying and pasting data from web pages into a local file.\n",
    "2. **Regular Expressions:** Using regex to match and extract patterns from HTML source code.\n",
    "3. **DOM Parsing:** Parsing HTML using browser-based Document Object Model (DOM) manipulation.\n",
    "4. **Libraries and Tools:** Using web scraping libraries and tools like Beautiful Soup, Scrapy, Selenium, and more.\n",
    "\n",
    "**Q3. What is Beautiful Soup? Why is it used?**\n",
    "- **Beautiful Soup:** Beautiful Soup is a Python library used for parsing HTML and XML documents. It provides tools for navigating and searching the parse tree, which represents the structure of the document.\n",
    "\n",
    "- **Use:** Beautiful Soup is used for web scraping to extract specific elements or data from HTML documents. It simplifies the process of navigating the HTML structure and locating the desired information.\n",
    "\n",
    "**Q4. Why is Flask used in this Web Scraping project?**\n",
    "Flask is a web framework that is used to build web applications and APIs. In the context of a web scraping project, Flask can be used to create a user-friendly interface for displaying the scraped data, allowing users to interact with the results or visualize the information in a structured way. Flask can serve as a backend for handling data requests, rendering templates, and serving the scraped data to users.\n",
    "\n",
    "**Q5. Write the names of AWS services used in this project. Also, explain the use of each service.**\n",
    "In the context of the provided questions, there's no direct indication of AWS services being used in the web scraping project. However, if you're referring to potential AWS services that could be used for hosting, storing, or processing the scraped data, here are a few examples:\n",
    "\n",
    "1. **Amazon EC2 (Elastic Compute Cloud):** EC2 is a scalable compute service that allows users to rent virtual servers (instances) for running applications. It could be used to host the Flask application that serves the scraped data.\n",
    "\n",
    "2. **Amazon S3 (Simple Storage Service):** S3 is a scalable object storage service for storing and retrieving data. It could be used to store the scraped data in a structured format.\n",
    "\n",
    "3. **Amazon RDS (Relational Database Service):** RDS is a managed database service for relational databases like MySQL, PostgreSQL, etc. It could be used to store structured data from the scraping process.\n",
    "\n",
    "4. **Amazon Lambda:** Lambda is a serverless compute service that runs code in response to events. It could be used to automate certain tasks in the web scraping process.\n",
    "\n",
    "5. **Amazon CloudWatch:** CloudWatch is a monitoring and observability service. It could be used to monitor the health and performance of the web scraping application.\n",
    "\n",
    "It's important to note that the specific AWS services used would depend on the requirements and architecture of the web scraping project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
